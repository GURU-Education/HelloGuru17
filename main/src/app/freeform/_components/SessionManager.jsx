// SessionManager.js
import { useState, useRef, useEffect } from "react";

export function useSessionManager(
  pronunciationAnalysisResult,
  selectedTopicData,
  selectedRole,
  startRecording,
  stopRecording,
  hskLevel,
  convoTopic,
  conversationList,
  splineObj, // Spline object for animations
  missionData = null // Optional mission data for freeform mode
) {
  const [isSessionActive, setIsSessionActive] = useState(false);
  const [events, setEvents] = useState([]);
  const [dataChannel, setDataChannel] = useState(null);
  const peerConnection = useRef(null);
  const audioElement = useRef(null);

  // Animation related refs
  const isMouthOpen = useRef(false);
  const audioContextRef = useRef(null);
  const animationRef = useRef(null);
  const serverAudioStreamRef = useRef(null);

  // Control mouth animations
  function openMouth() {
    if (!isMouthOpen.current && splineObj) {
      try {
        splineObj.emitEvent("keyDown", "Mouth");
        isMouthOpen.current = true;
      } catch (err) {
        console.error("Error opening mouth:", err);
      }
    }
  }

  function closeMouth() {
    if (isMouthOpen.current && splineObj) {
      try {
        splineObj.emitEvent("keyUp", "Mouth");
        isMouthOpen.current = false;
      } catch (err) {
        console.error("Error closing mouth:", err);
      }
    }
  }

  // Setup audio processing for mouth animation
  function setupAudioAnalysis(stream) {

    // Stop previous animation loop if running
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }

    // Close previous audio context if open
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
    }

    try {
      // Create new audio context and analyzer
      const audioContext = new (window.AudioContext ||
        window.webkitAudioContext)();
      audioContextRef.current = audioContext;

      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);
      const dataArray = new Uint8Array(analyser.frequencyBinCount);

      source.connect(analyser);
      // Don't connect to destination to avoid audio doubling

      function checkVolume() {
        analyser.getByteFrequencyData(dataArray);
        const volume = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

        // Open/close mouth based on volume threshold
        if (volume > 15 && !isMouthOpen.current) {
          openMouth();
        } else if (volume <= 15 && isMouthOpen.current) {
          closeMouth();
        }

        animationRef.current = requestAnimationFrame(checkVolume);
      }

      // Start volume checking
      audioContext.resume();
      checkVolume();
    } catch (err) {
      console.error("Error setting up audio analysis:", err);
    }
  }

  useEffect(() => {
    const handlePronunciationResult = async () => {
      if (pronunciationAnalysisResult) {
        await startSession();
      }
    };
    handlePronunciationResult();
  }, [pronunciationAnalysisResult]);

  useEffect(() => {
    if (pronunciationAnalysisResult && dataChannel) {
      dataChannel.addEventListener("message", (e) => {
        setEvents((prev) => [JSON.parse(e.data), ...prev]);
      });

      dataChannel.addEventListener("open", () => {
        setIsSessionActive(true);
        setEvents([]);
        const feedbackPrompt =
          "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡è€å¸ˆï¼Œå¸®åŠ©ä¸€ä½æ¯è¯­ä¸ºè‹±è¯­çš„å­¦ç”Ÿæé«˜æ™®é€šè¯å‘éŸ³ã€‚\n\n" +
          "è¯·åˆ†æžä»¥ä¸‹å‘éŸ³æ•°æ®ï¼Œå¹¶æä¾›**ç®€æ˜Žã€å®žç”¨**çš„åé¦ˆã€‚ä½ çš„åé¦ˆåº”åŒ…æ‹¬ï¼š\n" +
          "- **æŒ‡å‡ºé”™è¯¯**ï¼šåˆ—å‡ºå­¦ç”Ÿå‘éŸ³ä¸å‡†ç¡®çš„å­—æˆ–éŸ³èŠ‚ã€‚\n" +
          "- **æ­£ç¡®ç¤ºèŒƒ**ï¼šç”¨æ‹¼éŸ³å’Œæ±‰å­—å±•ç¤ºæ­£ç¡®å‘éŸ³ï¼Œå¹¶é™„ä¸Šç®€å•çš„ä¾‹å¥ã€‚\n" +
          "- **æ”¹è¿›å»ºè®®**ï¼šç”¨ç®€çŸ­ä¸­æ–‡è§£é‡Šå£åž‹ã€èˆŒä½æˆ–æ°”æµè°ƒæ•´æ–¹æ³•ï¼Œå¿…è¦æ—¶å¯ç”¨è‹±è¯­è¡¥å……ã€‚\n\n" +
          "### ç¤ºä¾‹åé¦ˆï¼š\n" +
          "- **è¯¯ (wÃ¹)**: ä½ è¯»æˆäº† *wÇ”*ï¼Œåº”è¯¥æ˜¯ **wÃ¹**ï¼ˆå››å£°ï¼‰ã€‚ðŸ”¹ ä¾‹å¥ï¼šã€Œæˆ‘é”™äº†ï¼Œæ˜¯æˆ‘çš„è¯¯ä¼šã€‚ã€\n" +
          "  - **ç»ƒä¹ **ï¼šä»Žé«˜éŸ³å¼€å§‹ï¼Œè¿…é€Ÿé™åˆ°ä½ŽéŸ³ï¼Œåƒæ„Ÿå¹ã€Œå“Žå‘€ã€çš„è¯­æ°”ã€‚\n" +
          "- **åƒ (chÄ«)**: ä½ çš„èˆŒå¤´ä½ç½®å¤ªé åŽã€‚ðŸ”¹ ä¾‹å¥ï¼šã€Œæˆ‘å–œæ¬¢åƒä¸­å›½èœã€‚ã€\n" +
          "  - **ç»ƒä¹ **ï¼šèˆŒå°–æŠµä½ä¸Šé¢šï¼Œè½»è½»é€æ°”ï¼Œä¸è¦å‘æˆ *ts* çš„éŸ³ã€‚\n\n" +
          "è¯·ç¡®ä¿ä½ çš„åé¦ˆ **ä»¥ä¸­æ–‡ä¸ºä¸»ï¼Œå¿…è¦æ—¶å°‘é‡è‹±æ–‡è§£é‡Š**ï¼Œä½¿å­¦ç”Ÿèƒ½ç›´æŽ¥ç†è§£å’Œç»ƒä¹ ã€‚\n\n" +
          "**ä»¥ä¸‹æ˜¯å­¦ç”Ÿçš„å‘éŸ³åˆ†æžæ•°æ®ï¼š**\n\n" +
          "```json\n" +
          JSON.stringify(pronunciationAnalysisResult, null, 2) +
          "\n```";
        sendTextMessage(feedbackPrompt);
      });
    }
  }, [pronunciationAnalysisResult, dataChannel]);

  useEffect(() => {
    if (dataChannel && !pronunciationAnalysisResult) {
      dataChannel.addEventListener("message", (e) => {
        setEvents((prev) => [JSON.parse(e.data), ...prev]);
      });

      dataChannel.addEventListener("open", () => {
        setIsSessionActive(true);
        setEvents([]);

        let promptText;

        // Check if we have mission data and use that for the prompt
        if (missionData && missionData.title) {
          // Create a mission-based prompt
          promptText =
            `You are a professional Chinese teacher guiding a native English-speaking student through a specific mission.\n\n` +
            `## Mission Context:\n` +
            `The mission title is: "${missionData.title}"\n\n` +
            `## Key Phrases for this Mission:\n`;

          // Add key phrases if available
          if (missionData.phrases && missionData.phrases.length > 0) {
            missionData.phrases.forEach((phrase) => {
              promptText += `- ${phrase.chinese} (${phrase.pinyin}): ${phrase.english}\n`;
            });
          }

          promptText +=
            `\n## Your Goal:\n` +
            `- Guide the student to correctly use these phrases in the context of this mission.\n` +
            `- Create a realistic conversational scenario where these phrases would be used.\n` +
            `- Provide gentle correction when the student makes mistakes.\n\n` +
            `## Teacher Role:\n` +
            `- **Guide the conversation**: proactively ask questions and guide the student to complete the mission.\n` +
            `- **Provide real-time feedback**: point out grammar or pronunciation errors in the student's speech, and offer concise suggestions for improvement.\n` +
            `- **Encourage open expression**: Allow students to express themselves freely, guiding gently and correcting significant errors without restricting their speech.\n\n` +
            `To begin, clearly explain the mission context and goal in Chinese to the student, then say:\n` +
            `ã€Œå‡†å¤‡å¥½äº†å—ï¼Ÿæˆ‘ä»¬å¼€å§‹å§ï¼ã€ ("Are you ready? Let's begin!")\n\n` +
            `Wait for the student's response before formally starting the conversation.`;
        } else {
          // Default roleplay script if no mission data is provided
          promptText =
            "We are role-playing. Your role is AA, the restaurant waiter (æœåŠ¡å‘˜), and I am BB, the customer (é¡¾å®¢). Speak VERY slow. I will read the customer's lines aloud, and you will read the waiter's lines aloud. Do not generate new text outside of the given script below.\n\n" +
            "AA: ä½ å¥½ï¼æ¬¢è¿Žå…‰ä¸´ï¼Œè¯·é—®å‡ ä½ï¼Ÿ\n" +
            "BB: ä½ å¥½ï¼Œå°±æˆ‘ä¸€ä¸ªäººã€‚è¯·ç»™æˆ‘ä¸€ä¸ªé çª—çš„åº§ä½ã€‚\n" +
            "AA: å¥½çš„ï¼Œè¯·è¿™è¾¹åã€‚è¿™æ˜¯èœå•ï¼Œè¯·æ…¢æ…¢çœ‹ã€‚\n" +
            "BB: è°¢è°¢ã€‚æˆ‘æƒ³ç‚¹å®«ä¿é¸¡ä¸å’Œä¸€ä»½ç±³é¥­ã€‚\n" +
            "AA: å¥½çš„ï¼Œè¯·ç¨ç­‰ï¼Œæ‚¨çš„èœå¾ˆå¿«å°±å¥½ã€‚\n" +
            "BB: å¥½çš„ï¼Œè°¢è°¢ã€‚\n\n";
        }

        // Send the appropriate prompt
        sendTextMessage(promptText);
      });
    }
  }, [dataChannel, missionData]);

  // Function to focus on a specific phrase (for freeform mode)
  const focusOnPhrase = (phrase) => {
    if (dataChannel && isSessionActive) {
      const promptText =
        `Let's practice this specific phrase: "${phrase.chinese}" (${phrase.pinyin}) which means "${phrase.english}".\n\n` +
        `Please provide specific guidance on the pronunciation and usage of this phrase. Give examples and ask me to repeat after you.`;

      sendTextMessage(promptText);
    }
  };

  async function startSession() {
    startRecording();

    try {
      const tokenResponse = await fetch("/api/token");
      const data = await tokenResponse.json();
      const EPHEMERAL_KEY = data.client_secret.value;

      const pc = new RTCPeerConnection();

      audioElement.current = document.createElement("audio");
      audioElement.current.autoplay = true;
      audioElement.current.muted = false;
      audioElement.current
        .play()
        .catch((error) => console.error("Autoplay blocked:", error));

      pc.ontrack = (e) => {
        console.log("Receiving AI audio stream:", e.streams[0]);

        // Save the stream reference for mouth animation
        serverAudioStreamRef.current = e.streams[0];

        // Apply the stream to the audio element
        audioElement.current.srcObject = e.streams[0];

        // Set up audio analysis for mouth movements
        if (splineObj) {
          setupAudioAnalysis(e.streams[0]);
        } else {
          console.warn("No Spline object available for mouth animation");
        }
      };

      const ms = await navigator.mediaDevices.getUserMedia({
        audio: true,
      });

      pc.addTrack(ms.getTracks()[0]);

      const dc = pc.createDataChannel("oai-events");
      setDataChannel(dc);

      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      const baseUrl = "https://api.openai.com/v1/realtime";
      const model = "gpt-4o-realtime-preview-2024-12-17";
      const sdpResponse = await fetch(`${baseUrl}?model=${model}`, {
        method: "POST",
        body: offer.sdp,
        headers: {
          Authorization: `Bearer ${EPHEMERAL_KEY}`,
          "Content-Type": "application/sdp",
        },
      });

      const answer = {
        type: "answer",
        sdp: await sdpResponse.text(),
      };
      await pc.setRemoteDescription(answer);

      peerConnection.current = pc;
    } catch (error) {
      console.error("Error starting session:", error);
    }
  }

  function stopSession() {
    // Stop analyzing audio for mouth movements
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }

    // Close audio context
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
    }

    // Close mouth if open
    if (isMouthOpen.current && splineObj) {
      closeMouth();
    }

    stopRecording();

    if (dataChannel) {
      dataChannel.close();
    }

    if (peerConnection.current) {
      peerConnection.current.getSenders().forEach((sender) => {
        if (sender.track) {
          sender.track.stop();
        }
      });
    }

    if (peerConnection.current) {
      peerConnection.current.close();
    }

    setIsSessionActive(false);
    setDataChannel(null);
    peerConnection.current = null;
    serverAudioStreamRef.current = null;
  }

  function sendClientEvent(message) {
    if (dataChannel) {
      message.event_id = message.event_id || crypto.randomUUID();
      dataChannel.send(JSON.stringify(message));
      setEvents((prev) => [message, ...prev]);
    } else {
      console.error(
        "Failed to send message - no data channel available",
        message
      );
    }
  }

  function sendTextMessage(message) {
    const event = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text: message,
          },
        ],
      },
    };

    sendClientEvent(event);
    sendClientEvent({ type: "response.create" });
  }

  return {
    startSession,
    stopSession,
    isSessionActive,
    events,
    focusOnPhrase, // Add the new function for phrase focusing
    sendTextMessage, // Expose sendTextMessage for direct interactions
  };
}
