// SessionManager.js
import { useState, useRef, useEffect } from "react";

export function useSessionManager(
  pronunciationAnalysisResult,
  selectedTopicData,
  selectedRole,
  startRecording,
  stopRecording,
  hskLevel,
  convoTopic,
  conversationList,
  splineObj, // Spline object for animations
  missionData = null // Optional mission data for mission mode
) {
  const [isSessionActive, setIsSessionActive] = useState(false);
  const [events, setEvents] = useState([]);
  const [dataChannel, setDataChannel] = useState(null);
  const peerConnection = useRef(null);
  const audioElement = useRef(null);

  // Animation related refs
  const isMouthOpen = useRef(false);
  const audioContextRef = useRef(null);
  const animationRef = useRef(null);
  const serverAudioStreamRef = useRef(null);

  // Control mouth animations
  function openMouth() {
    if (!isMouthOpen.current && splineObj) {
      try {
        splineObj.emitEvent("keyDown", "Mouth");
        isMouthOpen.current = true;
      } catch (err) {
        console.error("Error opening mouth:", err);
      }
    }
  }

  function closeMouth() {
    if (isMouthOpen.current && splineObj) {
      try {
        splineObj.emitEvent("keyUp", "Mouth");
        isMouthOpen.current = false;
      } catch (err) {
        console.error("Error closing mouth:", err);
      }
    }
  }

  // Setup audio processing for mouth animation
  function setupAudioAnalysis(stream) {

    // Stop previous animation loop if running
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }

    // Close previous audio context if open
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
    }

    try {
      // Create new audio context and analyzer
      const audioContext = new (window.AudioContext ||
        window.webkitAudioContext)();
      audioContextRef.current = audioContext;

      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);
      const dataArray = new Uint8Array(analyser.frequencyBinCount);

      source.connect(analyser);
      // Don't connect to destination to avoid audio doubling

      function checkVolume() {
        analyser.getByteFrequencyData(dataArray);
        const volume = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

        // Open/close mouth based on volume threshold
        if (volume > 15 && !isMouthOpen.current) {
          openMouth();
        } else if (volume <= 15 && isMouthOpen.current) {
          closeMouth();
        }

        animationRef.current = requestAnimationFrame(checkVolume);
      }

      // Start volume checking
      audioContext.resume();
      checkVolume();
    } catch (err) {
      console.error("Error setting up audio analysis:", err);
    }
  }

  useEffect(() => {
    const handlePronunciationResult = async () => {
      if (pronunciationAnalysisResult) {
        await startSession();
      }
    };
    handlePronunciationResult();
  }, [pronunciationAnalysisResult]);

  useEffect(() => {
    if (pronunciationAnalysisResult && dataChannel) {
      dataChannel.addEventListener("message", (e) => {
        setEvents((prev) => [JSON.parse(e.data), ...prev]);
      });

      dataChannel.addEventListener("open", () => {
        setIsSessionActive(true);
        setEvents([]);
        const feedbackPrompt =
          "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡è€å¸ˆï¼Œå¸®åŠ©ä¸€ä½æ¯è¯­ä¸ºè‹±è¯­çš„å­¦ç”Ÿæé«˜æ™®é€šè¯å‘éŸ³ã€‚\n\n" +
          "è¯·åˆ†æžä»¥ä¸‹å‘éŸ³æ•°æ®ï¼Œå¹¶æä¾›**ç®€æ˜Žã€å®žç”¨**çš„åé¦ˆã€‚ä½ çš„åé¦ˆåº”åŒ…æ‹¬ï¼š\n" +
          "- **æŒ‡å‡ºé”™è¯¯**ï¼šåˆ—å‡ºå­¦ç”Ÿå‘éŸ³ä¸å‡†ç¡®çš„å­—æˆ–éŸ³èŠ‚ã€‚\n" +
          "- **æ­£ç¡®ç¤ºèŒƒ**ï¼šç”¨æ‹¼éŸ³å’Œæ±‰å­—å±•ç¤ºæ­£ç¡®å‘éŸ³ï¼Œå¹¶é™„ä¸Šç®€å•çš„ä¾‹å¥ã€‚\n" +
          "- **æ”¹è¿›å»ºè®®**ï¼šç”¨ç®€çŸ­ä¸­æ–‡è§£é‡Šå£åž‹ã€èˆŒä½æˆ–æ°”æµè°ƒæ•´æ–¹æ³•ï¼Œå¿…è¦æ—¶å¯ç”¨è‹±è¯­è¡¥å……ã€‚\n\n" +
          "### ç¤ºä¾‹åé¦ˆï¼š\n" +
          "- **è¯¯ (wÃ¹)**: ä½ è¯»æˆäº† *wÇ”*ï¼Œåº”è¯¥æ˜¯ **wÃ¹**ï¼ˆå››å£°ï¼‰ã€‚ðŸ”¹ ä¾‹å¥ï¼šã€Œæˆ‘é”™äº†ï¼Œæ˜¯æˆ‘çš„è¯¯ä¼šã€‚ã€\n" +
          "  - **ç»ƒä¹ **ï¼šä»Žé«˜éŸ³å¼€å§‹ï¼Œè¿…é€Ÿé™åˆ°ä½ŽéŸ³ï¼Œåƒæ„Ÿå¹ã€Œå“Žå‘€ã€çš„è¯­æ°”ã€‚\n" +
          "- **åƒ (chÄ«)**: ä½ çš„èˆŒå¤´ä½ç½®å¤ªé åŽã€‚ðŸ”¹ ä¾‹å¥ï¼šã€Œæˆ‘å–œæ¬¢åƒä¸­å›½èœã€‚ã€\n" +
          "  - **ç»ƒä¹ **ï¼šèˆŒå°–æŠµä½ä¸Šé¢šï¼Œè½»è½»é€æ°”ï¼Œä¸è¦å‘æˆ *ts* çš„éŸ³ã€‚\n\n" +
          "è¯·ç¡®ä¿ä½ çš„åé¦ˆ **ä»¥ä¸­æ–‡ä¸ºä¸»ï¼Œå¿…è¦æ—¶å°‘é‡è‹±æ–‡è§£é‡Š**ï¼Œä½¿å­¦ç”Ÿèƒ½ç›´æŽ¥ç†è§£å’Œç»ƒä¹ ã€‚\n\n" +
          "**ä»¥ä¸‹æ˜¯å­¦ç”Ÿçš„å‘éŸ³åˆ†æžæ•°æ®ï¼š**\n\n" +
          "```json\n" +
          JSON.stringify(pronunciationAnalysisResult, null, 2) +
          "\n```";
        sendTextMessage(feedbackPrompt);
      });
    }
  }, [pronunciationAnalysisResult, dataChannel]);

  useEffect(() => {
    if (dataChannel && !pronunciationAnalysisResult) {
      dataChannel.addEventListener("message", (e) => {
        setEvents((prev) => [JSON.parse(e.data), ...prev]);
      });

      dataChannel.addEventListener("open", () => {
        setIsSessionActive(true);
        setEvents([]);

        let promptText;

        // Check if we have mission data and use that for the prompt
        // Create a mission-based prompt
        promptText =
          `You are a professional Chinese teacher guiding a native English-speaking student through a specific mission.\n\n` +
          `## Mission Context:\n` +
          `The mission title is: "${missionData.title}"\n\n` +
          `## Key Phrases for this Mission:\n`;

          // Add key phrases if available
          const demoScript = [
            { speaker: "AI", mood: "Cheerful", text: "Hey Tarin! å¥½ä¹…ä¸è§ (hÇŽo jiÇ” bÃº jiÃ n)! Howâ€™s my favorite student doing today?" },
            { speaker: "Student", mood: "Casual", text: "Hey XiaoQiu, I'm pretty good, thanks! Ready for another lesson." },
            { speaker: "AI", mood: "Excited", text: "Awesome! ä»Šå¤©æˆ‘ä»¬è¦èŠèŠæ—…æ¸¸ (lÇšyÃ³u), traveling! I know you mentioned last time that you're excited to travel more." },
            { speaker: "Student", mood: "Enthusiastic", text: "Yeah, totally! I've actually been thinking about planning a trip soon, maybe to ä¸­å›½ or éŸ©å›½." },
            { speaker: "AI", mood: "Encouraging", text: "Great! Let's start with this word: æ—…è¡Œ (lÇšxÃ­ng), meaning 'travel.' Repeat after me: æ—…è¡Œ (lÇšxÃ­ng)." },
            { speaker: "Student", mood: "Attempting", text: "lÇ”xing1." },
            { speaker: "AI", mood: "Supportive", text: "Almost! Pay attention to your pronunciation and tone. You said 'lÇ”xing1,' but it's actually 'lÇšxÃ­ng.' Try again?" },
            { speaker: "Student", mood: "Retrying", text: "æ—…è¡Œ (lÇšxÃ­ng)." },
            { speaker: "AI", mood: "Encouraging", text: "Perfect! Much better. Much better. Now, can you use it in a sentence now" },
            { speaker: "Student", mood: "Curious", text: "Actually, XiaoQiu, what's the difference between æ—…æ¸¸ (lÇšyÃ³u) and æ—…è¡Œ (lÇšxÃ­ng)?" },
            { speaker: "AI", mood: "Explaining", text: "Good question! æ—…è¡Œ generally means any kind of travel or journey. æ—…æ¸¸, on the other hand, specifically refers to leisure travel, sightseeing, or vacation." },
            { speaker: "Student", mood: "Understanding", text: "Ah, that clears it up." },
            { speaker: "AI", mood: "Joking", text: "Exactly! So, a quick trip to your fridge for snacks wouldn't count as æ—…æ¸¸, that's just a short æ—…è¡Œ." },
            { speaker: "Student", mood: "Laughing", text: "Very funny, XiaoQiu." },
            { speaker: "AI", mood: "Teasing", text: "Remember last lesson when you struggled with é…’åº— (jiÇ”diÃ n), meaning hotel? Want to give it another shot?" },
            { speaker: "Student", mood: "Confident", text: "é…’åº— (jiÇ”diÃ n)." },
            { speaker: "AI", mood: "Surprised and praising", text: "Excellent pronunciation! You've definitely improved since last time." },
            { speaker: "Student", mood: "Playful", text: "Yeah, I practiced a lot since last time." },
            { speaker: "AI", mood: "Laughing", text: "I can tell! Great job. Now, can you try making a sentence using æ—…è¡Œ (lÇšxÃ­ng)?" },
            { speaker: "Student", mood: "Thinking", text: "å—¯...æˆ‘æƒ³åŽ»ä¸­å›½æ—…è¡Œå’Œæœ‹å‹ä¸€èµ·ã€‚(wÇ’ xiÇŽng qÃ¹ zhÅngguÃ³ lÇšxÃ­ng hÃ© pÃ©ngyou yÃ¬qÇ)." },
            { speaker: "AI", mood: "Gently correcting", text: "Nice effort! Your idea is clear, but the word order needs a slight adjustment. It sounds more natural to say: æˆ‘æƒ³å’Œæœ‹å‹ä¸€èµ·åŽ»ä¸­å›½æ—…è¡Œ (wÇ’ xiÇŽng hÃ© pÃ©ngyou yÃ¬qÇ qÃ¹ zhÅngguÃ³ lÇšxÃ­ng)." },
            { speaker: "Student", mood: "Retrying", text: "æˆ‘æƒ³å’Œæœ‹å‹ä¸€èµ·åŽ»ä¸­å›½æ—…è¡Œã€‚(wÇ’ xiÇŽng hÃ© pÃ©ngyou yÃ¬qÇ qÃ¹ zhÅngguÃ³ lÇšxÃ­ng)." },
            { speaker: "AI", mood: "Praising", text: "Perfect! That sounds great. You're really improving!" },
            { speaker: "Student", mood: "Playful", text: "XiaoQiu, can you teach me something fun I can say to my friends?" },
            { speaker: "AI", mood: "Excited", text: "Of course! How about: ä¸–ç•Œè¿™ä¹ˆå¤§ï¼Œæˆ‘æƒ³åŽ»çœ‹çœ‹ã€‚(shÃ¬jiÃ¨ zhÃ¨me dÃ , wÇ’ xiÇŽng qÃ¹ kÃ nkan) â€“ 'The world is so big, I want to see it!' Pretty cool, huh?" },
            { speaker: "Student", mood: "Practicing", text: "ä¸–ç•Œè¿™ä¹ˆå¤§ï¼Œæˆ‘æƒ³åŽ»çœ‹çœ‹ã€‚(shÃ¬jiÃ¨ zhÃ¨me dÃ , wÇ’ xiÇŽng qÃ¹ kÃ nkan)" },
            { speaker: "AI", mood: "Praising", text: "Excellent! Now you're ready to impress your friends." },
            { speaker: "AI", mood: "Joking", text: "Just make sure you actually go somewhere interestingâ€”beyond the kitchen!" },
            { speaker: "Student", mood: "Laughing", text: "Got it è°¢è°¢ Xiaoqiu! " },
            { speaker: "AI", mood: "Playful", text: "ä¸å®¢æ°” tarin! great job today. ä¸‹æ¬¡è§ (xiÃ  cÃ¬ jiÃ n)! See you next lesson, and don't forget to revise for the next leason" }
        ];

        const instructionScript = "Instruction for AI: You're XiaoQiu, a fun, energetic, patient Chinese tutor guiding Tarin (HSK Level 3). Strictly follow this demoScript during interaction. Provide pronunciation and grammar feedback exactly as scripted. Keep interactions playful, humorous, and engaging. Explicitly call the trackProgress function as scripted. \n\n"
        
        const scriptToString = demoScript.map(line => {
            const actionText = line.action ? ` [Action: ${line.action}]` : '';
            return `${line.speaker} (${line.mood}): ${line.text}${actionText}`;
        }).join('\n');

        const prompt = instructionScript + "\n\n" + scriptToString;
        

        // Send the appropriate prompt
        sendTextMessage(prompt);
    }, [dataChannel, missionData]) 
  }})

  // Function to focus on a specific phrase (for mission mode)
  const focusOnPhrase = (phrase) => {
    if (dataChannel && isSessionActive) {
      const promptText =
        `Let's practice this specific phrase: "${phrase.chinese}" (${phrase.pinyin}) which means "${phrase.english}".\n\n` +
        `Please provide specific guidance on the pronunciation and usage of this phrase. Give examples and ask me to repeat after you.`;

      sendTextMessage(promptText);
    }
  };

  async function startSession() {
    startRecording();

    try {
      const tokenResponse = await fetch("/api/token");
      const data = await tokenResponse.json();
      const EPHEMERAL_KEY = data.client_secret.value;

      const pc = new RTCPeerConnection();

      audioElement.current = document.createElement("audio");
      audioElement.current.autoplay = true;
      audioElement.current.muted = false;
      audioElement.current
        .play()
        .catch((error) => console.error("Autoplay blocked:", error));

      pc.ontrack = (e) => {
        console.log("Receiving AI audio stream:", e.streams[0]);

        // Save the stream reference for mouth animation
        serverAudioStreamRef.current = e.streams[0];

        // Apply the stream to the audio element
        audioElement.current.srcObject = e.streams[0];

        // Set up audio analysis for mouth movements
        if (splineObj) {
          setupAudioAnalysis(e.streams[0]);
        } else {
          console.warn("No Spline object available for mouth animation");
        }
      };

      const ms = await navigator.mediaDevices.getUserMedia({
        audio: true,
      });

      pc.addTrack(ms.getTracks()[0]);

      const dc = pc.createDataChannel("oai-events");
      setDataChannel(dc);

      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      const baseUrl = "https://api.openai.com/v1/realtime";
      const model = "gpt-4o-realtime-preview-2024-12-17";
      const sdpResponse = await fetch(`${baseUrl}?model=${model}`, {
        method: "POST",
        body: offer.sdp,
        headers: {
          Authorization: `Bearer ${EPHEMERAL_KEY}`,
          "Content-Type": "application/sdp",
        },
      });

      const answer = {
        type: "answer",
        sdp: await sdpResponse.text(),
      };
      await pc.setRemoteDescription(answer);

      peerConnection.current = pc;
    } catch (error) {
      console.error("Error starting session:", error);
    }
  }

  function stopSession() {
    // Stop analyzing audio for mouth movements
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }

    // Close audio context
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
    }

    // Close mouth if open
    if (isMouthOpen.current && splineObj) {
      closeMouth();
    }

    stopRecording();

    if (dataChannel) {
      dataChannel.close();
    }

    if (peerConnection.current) {
      peerConnection.current.getSenders().forEach((sender) => {
        if (sender.track) {
          sender.track.stop();
        }
      });
    }

    if (peerConnection.current) {
      peerConnection.current.close();
    }

    setIsSessionActive(false);
    setDataChannel(null);
    peerConnection.current = null;
    serverAudioStreamRef.current = null;
  }

  function sendClientEvent(message) {
    if (dataChannel) {
      message.event_id = message.event_id || crypto.randomUUID();
      dataChannel.send(JSON.stringify(message));
      setEvents((prev) => [message, ...prev]);
    } else {
      console.error(
        "Failed to send message - no data channel available",
        message
      );
    }
  }

  function sendTextMessage(message) {
    const event = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text: message,
          },
        ],
      },
    };

    sendClientEvent(event);
    sendClientEvent({ type: "response.create" });
  }

  return {
    startSession,
    stopSession,
    isSessionActive,
    events,
    focusOnPhrase, // Add the new function for phrase focusing
    sendTextMessage, // Expose sendTextMessage for direct interactions
  };
}
